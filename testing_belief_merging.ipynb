{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB7F6HNi2V3a",
        "outputId": "a762d235-6998-48ea-8295-aabcae85dd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/ViFLAVOR_Belief-merging/data_test.zip -d /content/\n",
        "!pip install unidecode ultralytics==8.2.0"
      ],
      "metadata": {
        "id": "cwNQV4uaMmNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "print(ultralytics.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yUbkeNDEGH3",
        "outputId": "fe6d01f8-5ddd-4e52-c4cc-42176e2ce4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief Merging of Max"
      ],
      "metadata": {
        "id": "h9CrWipM6GNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO, RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/classes.txt'\n",
        "excel_file_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_yolov8 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov8_best_862.pt')\n",
        "model_yolov9 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov9_best_85.pt')\n",
        "model_rtdetr = RTDETR('/content/drive/MyDrive/ViFLAVOR_Belief-merging/rt-detr_best_865.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "\n",
        "    # predicted_food = []\n",
        "    # for min_index in min_indices:\n",
        "    #     predicted_food.append(df.loc[min_index, \"Food\"])\n",
        "\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "def calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances):\n",
        "    max_list = []\n",
        "    sum_list = []\n",
        "    Gmax = []\n",
        "\n",
        "    for index, v8_value in enumerate(v8_distances):\n",
        "        v9_value = v9_distances[index]\n",
        "        rtdetr_value = rtdetr_distances[index]\n",
        "\n",
        "        max_value = min(v8_value, v9_value, rtdetr_value)\n",
        "        sum_value = v8_value + v9_value + rtdetr_value\n",
        "        numbers = [math.ceil(v8_value), math.ceil(v9_value), math.ceil(rtdetr_value)]\n",
        "        numbers = sorted(numbers)\n",
        "        gmax = ' '.join(map(str, numbers))\n",
        "\n",
        "        max_list.append(max_value)\n",
        "        sum_list.append(sum_value)\n",
        "        Gmax.append(gmax)\n",
        "\n",
        "    return max_list, sum_list, Gmax\n",
        "\n",
        "def find_gmax_indices(arr):\n",
        "    gmax_indices = [0]  # Bắt đầu với phần tử đầu tiên\n",
        "\n",
        "    for i in range(1, len(arr)):\n",
        "        for j in range(len(arr[0])):\n",
        "            if arr[i][j] < arr[gmax_indices[0]][j]:\n",
        "                gmax_indices = [i]\n",
        "                break\n",
        "            elif arr[i][j] > arr[gmax_indices[0]][j]:\n",
        "                break\n",
        "            elif j == len(arr[0]) - 1:  # Khi toàn bộ các phần tử đều bằng nhau\n",
        "                gmax_indices.append(i)\n",
        "\n",
        "    return gmax_indices\n",
        "\n",
        "\n",
        "def belief_merging_find_foodname(max_list, sum, Gmax):\n",
        "    global df\n",
        "\n",
        "    min_value_of_maxlist = min(max_list)\n",
        "    indices_of_maxlist = [index for index, value in enumerate(max_list) if value == min_value_of_maxlist]\n",
        "\n",
        "    min_value_of_sum = min(sum)\n",
        "    indices_of_sum = [index for index, value in enumerate(sum) if value == min_value_of_sum]\n",
        "\n",
        "    gmax_values = [\n",
        "        list(map(int, item.split())) for item in Gmax\n",
        "    ]\n",
        "    indices_of_gmax = find_gmax_indices(gmax_values)\n",
        "\n",
        "    return indices_of_maxlist[0], indices_of_sum[0], indices_of_gmax[0]\n",
        "\n",
        "predicted_max_belief_merging = []\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global predicted_max_belief_merging\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    yolov8_names, yolov8_detected_cls, yolov8_boxes = detect_ingredients(model_yolov8, img)\n",
        "    yolov9_names, yolov9_detected_cls, yolov9_boxes = detect_ingredients(model_yolov9, img)\n",
        "    rtdetr_names, rtdetr_detected_cls, rtdetr_boxes = detect_ingredients(model_rtdetr, img)\n",
        "\n",
        "    v8_rm_duplicates = set(yolov8_detected_cls)\n",
        "    v8_rm_duplicates = [int(number) for number in v8_rm_duplicates]\n",
        "    yolov8_detected_cls = list(v8_rm_duplicates)\n",
        "    v8_rm_duplicates = convertIdx2Class(v8_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v9_rm_duplicates = set(yolov9_detected_cls)\n",
        "    v9_rm_duplicates = [int(number) for number in v9_rm_duplicates]\n",
        "    yolov9_detected_cls = list(v9_rm_duplicates)\n",
        "    v9_rm_duplicates = convertIdx2Class(v9_rm_duplicates, classes_ingre)\n",
        "\n",
        "    rtdetr_rm_duplicates = set(rtdetr_detected_cls)\n",
        "    rtdetr_rm_duplicates = [int(number) for number in rtdetr_rm_duplicates]\n",
        "    rtdetr_detected_cls = list(rtdetr_rm_duplicates)\n",
        "    rtdetr_rm_duplicates = convertIdx2Class(rtdetr_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v8_distances, v8_predicted_food = find_foodname_ver_narrow(v8_rm_duplicates)\n",
        "    v9_distances, v9_predicted_food = find_foodname_ver_narrow(v9_rm_duplicates)\n",
        "    rtdetr_distances, rtdetr_predicted_food = find_foodname_ver_narrow(rtdetr_rm_duplicates)\n",
        "    print(v8_predicted_food, v9_predicted_food, rtdetr_predicted_food)\n",
        "\n",
        "    max_list, sum_list, Gmax = calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances)\n",
        "\n",
        "    max_predicted_food, sum_predicted_food, gmax_predicted_food = belief_merging_find_foodname(max_list, sum_list, Gmax)\n",
        "    print(max_predicted_food)\n",
        "\n",
        "    # Kết quả\n",
        "    # print(\"Detected Classes YOLOv8:\", yolov8_detected_cls)\n",
        "    # print(\"Detected Classes YOLOv9:\", yolov9_detected_cls)\n",
        "    # print(\"Detected Classes RTDETR:\", rtdetr_detected_cls)\n",
        "    # print(\"Predicted Food YOLOv8:\", v8_predicted_food)\n",
        "    # print(\"Predicted Food YOLOv9:\", v9_predicted_food)\n",
        "    # print(\"Predicted Food RTDETR:\", rtdetr_predicted_food)\n",
        "    # print(\"Max List:\", max_list)\n",
        "    # print(\"Sum List:\", sum_list)\n",
        "    # print(\"Gmax:\", Gmax)\n",
        "    # print(\"Max of Belief Merging Predicted Food:\", max_predicted_food)\n",
        "    # print(\"Sum of Belief Merging Predicted Food:\", sum_predicted_food)\n",
        "    # print(\"Gmax of Belief Merging Predicted Food:\", gmax_predicted_food)\n",
        "\n",
        "    predicted_max_belief_merging.append(max_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue',\n",
        "               'pho-ha-noi', 'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/data_test/'+category):\n",
        "            imagePaths.append(['/content/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)"
      ],
      "metadata": {
        "id": "HtgdHT_w83iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_max_belief_merging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYMm7lYUNWlr",
        "outputId": "999c559f-d223-4a1d-bad3-41c8db6b5f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 11, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, predicted_max_belief_merging)\n",
        "precision = precision_score(labels, predicted_max_belief_merging, average='weighted')\n",
        "recall = recall_score(labels, predicted_max_belief_merging, average='weighted')\n",
        "f1 = f1_score(labels, predicted_max_belief_merging, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, predicted_max_belief_merging, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, predicted_max_belief_merging)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioz9tXwGvBSK",
        "outputId": "1179d282-6ee3-4c05-dedf-eab92dbf55ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.0\n",
            "Precision: 95.96675105556683\n",
            "Recall: 95.0\n",
            "F1 Score: 95.10498079181137\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.67      1.00      0.80        20\n",
            "    hu-tieu-my-tho       1.00      0.86      0.92        14\n",
            "      bun-nuoc-leo       1.00      0.73      0.84        26\n",
            "com-tam-long-xuyen       1.00      1.00      1.00        36\n",
            " bun-hai-san-be-be       1.00      0.93      0.97        30\n",
            " banh-hoi-heo-quay       0.94      1.00      0.97        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.97      0.95      0.96        37\n",
            "          mi-quang       0.89      0.94      0.91        33\n",
            "        bun-bo-hue       1.00      0.97      0.99        36\n",
            "        pho-ha-noi       0.97      1.00      0.99        36\n",
            "           bun-muc       0.92      0.92      0.92        13\n",
            "           bun-moc       0.95      1.00      0.97        19\n",
            "   bun-dau-mam-tom       1.00      0.92      0.96        25\n",
            "\n",
            "          accuracy                           0.95       380\n",
            "         macro avg       0.95      0.94      0.94       380\n",
            "      weighted avg       0.96      0.95      0.95       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.86\n",
            "bun-nuoc-leo: 0.73\n",
            "com-tam-long-xuyen: 1.00\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.95\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 0.97\n",
            "pho-ha-noi: 1.00\n",
            "bun-muc: 0.92\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief Merging of Sum"
      ],
      "metadata": {
        "id": "gJruSYLG66N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO, RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/classes.txt'\n",
        "excel_file_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_yolov8 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov8_best_862.pt')\n",
        "model_yolov9 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov9_best_85.pt')\n",
        "model_rtdetr = RTDETR('/content/drive/MyDrive/ViFLAVOR_Belief-merging/rt-detr_best_865.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "def calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances):\n",
        "    max_list = []\n",
        "    sum_list = []\n",
        "    Gmax = []\n",
        "\n",
        "    for index, v8_value in enumerate(v8_distances):\n",
        "        v9_value = v9_distances[index]\n",
        "        rtdetr_value = rtdetr_distances[index]\n",
        "\n",
        "        max_value = min(v8_value, v9_value, rtdetr_value)\n",
        "        sum_value = v8_value + v9_value + rtdetr_value\n",
        "        numbers = [math.ceil(v8_value), math.ceil(v9_value), math.ceil(rtdetr_value)]\n",
        "        numbers = sorted(numbers)\n",
        "        gmax = ' '.join(map(str, numbers))\n",
        "\n",
        "        max_list.append(max_value)\n",
        "        sum_list.append(sum_value)\n",
        "        Gmax.append(gmax)\n",
        "\n",
        "    return max_list, sum_list, Gmax\n",
        "\n",
        "def find_gmax_indices(arr):\n",
        "    gmax_indices = [0]  # Bắt đầu với phần tử đầu tiên\n",
        "\n",
        "    for i in range(1, len(arr)):\n",
        "        for j in range(len(arr[0])):\n",
        "            if arr[i][j] < arr[gmax_indices[0]][j]:\n",
        "                gmax_indices = [i]\n",
        "                break\n",
        "            elif arr[i][j] > arr[gmax_indices[0]][j]:\n",
        "                break\n",
        "            elif j == len(arr[0]) - 1:  # Khi toàn bộ các phần tử đều bằng nhau\n",
        "                gmax_indices.append(i)\n",
        "\n",
        "    return gmax_indices\n",
        "\n",
        "\n",
        "def belief_merging_find_foodname(max_list, sum, Gmax):\n",
        "    global df\n",
        "\n",
        "    min_value_of_maxlist = min(max_list)\n",
        "    indices_of_maxlist = [index for index, value in enumerate(max_list) if value == min_value_of_maxlist]\n",
        "\n",
        "    min_value_of_sum = min(sum)\n",
        "    indices_of_sum = [index for index, value in enumerate(sum) if value == min_value_of_sum]\n",
        "\n",
        "    gmax_values = [\n",
        "        list(map(int, item.split())) for item in Gmax\n",
        "    ]\n",
        "    indices_of_gmax = find_gmax_indices(gmax_values)\n",
        "\n",
        "    return indices_of_maxlist[0], indices_of_sum[0], indices_of_gmax[0]\n",
        "\n",
        "predicted_sum_belief_merging = []\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global predicted_sum_belief_merging\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    yolov8_names, yolov8_detected_cls, yolov8_boxes = detect_ingredients(model_yolov8, img)\n",
        "    yolov9_names, yolov9_detected_cls, yolov9_boxes = detect_ingredients(model_yolov9, img)\n",
        "    rtdetr_names, rtdetr_detected_cls, rtdetr_boxes = detect_ingredients(model_rtdetr, img)\n",
        "\n",
        "    v8_rm_duplicates = set(yolov8_detected_cls)\n",
        "    v8_rm_duplicates = [int(number) for number in v8_rm_duplicates]\n",
        "    yolov8_detected_cls = list(v8_rm_duplicates)\n",
        "    v8_rm_duplicates = convertIdx2Class(v8_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v9_rm_duplicates = set(yolov9_detected_cls)\n",
        "    v9_rm_duplicates = [int(number) for number in v9_rm_duplicates]\n",
        "    yolov9_detected_cls = list(v9_rm_duplicates)\n",
        "    v9_rm_duplicates = convertIdx2Class(v9_rm_duplicates, classes_ingre)\n",
        "\n",
        "    rtdetr_rm_duplicates = set(rtdetr_detected_cls)\n",
        "    rtdetr_rm_duplicates = [int(number) for number in rtdetr_rm_duplicates]\n",
        "    rtdetr_detected_cls = list(rtdetr_rm_duplicates)\n",
        "    rtdetr_rm_duplicates = convertIdx2Class(rtdetr_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v8_distances, v8_predicted_food = find_foodname_ver_narrow(v8_rm_duplicates)\n",
        "    v9_distances, v9_predicted_food = find_foodname_ver_narrow(v9_rm_duplicates)\n",
        "    rtdetr_distances, rtdetr_predicted_food = find_foodname_ver_narrow(rtdetr_rm_duplicates)\n",
        "    print(v8_predicted_food, v9_predicted_food, rtdetr_predicted_food)\n",
        "\n",
        "    max_list, sum_list, Gmax = calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances)\n",
        "\n",
        "    max_predicted_food, sum_predicted_food, gmax_predicted_food = belief_merging_find_foodname(max_list, sum_list, Gmax)\n",
        "\n",
        "    # print(\"Max of Belief Merging Predicted Food:\", max_predicted_food)\n",
        "    # print(\"Sum of Belief Merging Predicted Food:\", sum_predicted_food)\n",
        "    # print(\"Gmax of Belief Merging Predicted Food:\", gmax_predicted_food)\n",
        "\n",
        "    predicted_sum_belief_merging.append(sum_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue', 'pho-ha-noi',\n",
        "               'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/data_test/'+category):\n",
        "            imagePaths.append(['/content/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)\n"
      ],
      "metadata": {
        "id": "VU9IEd8N6M9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_sum_belief_merging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpI6R3SR7IEY",
        "outputId": "2f8ded8c-009e-4f1c-8d3d-b1b0b076afff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, predicted_sum_belief_merging)\n",
        "precision = precision_score(labels, predicted_sum_belief_merging, average='weighted')\n",
        "recall = recall_score(labels, predicted_sum_belief_merging, average='weighted')\n",
        "f1 = f1_score(labels, predicted_sum_belief_merging, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, predicted_sum_belief_merging, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, predicted_sum_belief_merging)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhXKQn247Hdm",
        "outputId": "d67c434e-d8d2-4e0f-b816-c519c37d06af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.47368421052632\n",
            "Precision: 95.59807917650716\n",
            "Recall: 94.47368421052632\n",
            "F1 Score: 94.63417173516093\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.67      1.00      0.80        20\n",
            "    hu-tieu-my-tho       1.00      0.86      0.92        14\n",
            "      bun-nuoc-leo       1.00      0.77      0.87        26\n",
            "com-tam-long-xuyen       1.00      1.00      1.00        36\n",
            " bun-hai-san-be-be       1.00      0.93      0.97        30\n",
            " banh-hoi-heo-quay       0.94      1.00      0.97        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.97      0.86      0.91        37\n",
            "          mi-quang       0.82      0.94      0.87        33\n",
            "        bun-bo-hue       1.00      0.97      0.99        36\n",
            "        pho-ha-noi       0.97      1.00      0.99        36\n",
            "           bun-muc       1.00      0.92      0.96        13\n",
            "           bun-moc       0.95      1.00      0.97        19\n",
            "   bun-dau-mam-tom       1.00      0.92      0.96        25\n",
            "\n",
            "          accuracy                           0.94       380\n",
            "         macro avg       0.95      0.94      0.94       380\n",
            "      weighted avg       0.96      0.94      0.95       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.86\n",
            "bun-nuoc-leo: 0.77\n",
            "com-tam-long-xuyen: 1.00\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.86\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 0.97\n",
            "pho-ha-noi: 1.00\n",
            "bun-muc: 0.92\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief Merging of Gmax"
      ],
      "metadata": {
        "id": "vmfGip089IdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO, RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/classes.txt'\n",
        "excel_file_path = '/content/drive/MyDrive/ViFLAVOR_Belief-merging/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_yolov8 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov8_best_862.pt')\n",
        "model_yolov9 = YOLO('/content/drive/MyDrive/ViFLAVOR_Belief-merging/yolov9_best_85.pt')\n",
        "model_rtdetr = RTDETR('/content/drive/MyDrive/ViFLAVOR_Belief-merging/rt-detr_best_865.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "def calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances):\n",
        "    max_list = []\n",
        "    sum_list = []\n",
        "    Gmax = []\n",
        "\n",
        "    for index, v8_value in enumerate(v8_distances):\n",
        "        v9_value = v9_distances[index]\n",
        "        rtdetr_value = rtdetr_distances[index]\n",
        "\n",
        "        max_value = min(v8_value, v9_value, rtdetr_value)\n",
        "        sum_value = v8_value + v9_value + rtdetr_value\n",
        "        numbers = [math.ceil(v8_value), math.ceil(v9_value), math.ceil(rtdetr_value)]\n",
        "        numbers = sorted(numbers)\n",
        "        gmax = ' '.join(map(str, numbers))\n",
        "\n",
        "        max_list.append(max_value)\n",
        "        sum_list.append(sum_value)\n",
        "        Gmax.append(gmax)\n",
        "\n",
        "    return max_list, sum_list, Gmax\n",
        "\n",
        "def find_gmax_indices(arr):\n",
        "    gmax_indices = [0]  # Bắt đầu với phần tử đầu tiên\n",
        "\n",
        "    for i in range(1, len(arr)):\n",
        "        for j in range(len(arr[0])):\n",
        "            if arr[i][j] < arr[gmax_indices[0]][j]:\n",
        "                gmax_indices = [i]\n",
        "                break\n",
        "            elif arr[i][j] > arr[gmax_indices[0]][j]:\n",
        "                break\n",
        "            elif j == len(arr[0]) - 1:  # Khi toàn bộ các phần tử đều bằng nhau\n",
        "                gmax_indices.append(i)\n",
        "\n",
        "    return gmax_indices\n",
        "\n",
        "\n",
        "def belief_merging_find_foodname(max_list, sum, Gmax):\n",
        "    global df\n",
        "\n",
        "    min_value_of_maxlist = min(max_list)\n",
        "    indices_of_maxlist = [index for index, value in enumerate(max_list) if value == min_value_of_maxlist]\n",
        "\n",
        "    min_value_of_sum = min(sum)\n",
        "    indices_of_sum = [index for index, value in enumerate(sum) if value == min_value_of_sum]\n",
        "\n",
        "    gmax_values = [\n",
        "        list(map(int, item.split())) for item in Gmax\n",
        "    ]\n",
        "    indices_of_gmax = find_gmax_indices(gmax_values)\n",
        "\n",
        "    return indices_of_maxlist[0], indices_of_sum[0], indices_of_gmax[0]\n",
        "\n",
        "predicted_gmax_belief_merging = []\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global predicted_gmax_belief_merging\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    yolov8_names, yolov8_detected_cls, yolov8_boxes = detect_ingredients(model_yolov8, img)\n",
        "    yolov9_names, yolov9_detected_cls, yolov9_boxes = detect_ingredients(model_yolov9, img)\n",
        "    rtdetr_names, rtdetr_detected_cls, rtdetr_boxes = detect_ingredients(model_rtdetr, img)\n",
        "\n",
        "    v8_rm_duplicates = set(yolov8_detected_cls)\n",
        "    v8_rm_duplicates = [int(number) for number in v8_rm_duplicates]\n",
        "    yolov8_detected_cls = list(v8_rm_duplicates)\n",
        "    v8_rm_duplicates = convertIdx2Class(v8_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v9_rm_duplicates = set(yolov9_detected_cls)\n",
        "    v9_rm_duplicates = [int(number) for number in v9_rm_duplicates]\n",
        "    yolov9_detected_cls = list(v9_rm_duplicates)\n",
        "    v9_rm_duplicates = convertIdx2Class(v9_rm_duplicates, classes_ingre)\n",
        "\n",
        "    rtdetr_rm_duplicates = set(rtdetr_detected_cls)\n",
        "    rtdetr_rm_duplicates = [int(number) for number in rtdetr_rm_duplicates]\n",
        "    rtdetr_detected_cls = list(rtdetr_rm_duplicates)\n",
        "    rtdetr_rm_duplicates = convertIdx2Class(rtdetr_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v8_distances, v8_predicted_food = find_foodname_ver_narrow(v8_rm_duplicates)\n",
        "    v9_distances, v9_predicted_food = find_foodname_ver_narrow(v9_rm_duplicates)\n",
        "    rtdetr_distances, rtdetr_predicted_food = find_foodname_ver_narrow(rtdetr_rm_duplicates)\n",
        "\n",
        "    max_list, sum_list, Gmax = calculate_belief_merging_base(v8_distances, v9_distances, rtdetr_distances)\n",
        "\n",
        "    max_predicted_food, sum_predicted_food, gmax_predicted_food = belief_merging_find_foodname(max_list, sum_list, Gmax)\n",
        "\n",
        "    # print(\"Max of Belief Merging Predicted Food:\", max_predicted_food)\n",
        "    # print(\"Sum of Belief Merging Predicted Food:\", sum_predicted_food)\n",
        "    # print(\"Gmax of Belief Merging Predicted Food:\", gmax_predicted_food)\n",
        "\n",
        "    predicted_gmax_belief_merging.append(gmax_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue', 'pho-ha-noi',\n",
        "               'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/data_test/'+category):\n",
        "            imagePaths.append(['/content/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)\n"
      ],
      "metadata": {
        "id": "d-btYYu09H1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_gmax_belief_merging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA6U5ucl9dTb",
        "outputId": "4b9ad77a-cddc-4e9c-91be-ad417048b44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 8, 1, 1, 1, 8, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 0, 11, 11, 11, 4, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, predicted_gmax_belief_merging)\n",
        "precision = precision_score(labels, predicted_gmax_belief_merging, average='weighted')\n",
        "recall = recall_score(labels, predicted_gmax_belief_merging, average='weighted')\n",
        "f1 = f1_score(labels, predicted_gmax_belief_merging, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, predicted_gmax_belief_merging, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, predicted_gmax_belief_merging)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1z_UiVe9kZf",
        "outputId": "90ff7ed4-25ec-48c0-ee1c-58de19a35ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.73684210526315\n",
            "Precision: 95.84449045411884\n",
            "Recall: 94.73684210526315\n",
            "F1 Score: 94.87129663556867\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.65      1.00      0.78        20\n",
            "    hu-tieu-my-tho       1.00      0.86      0.92        14\n",
            "      bun-nuoc-leo       1.00      0.73      0.84        26\n",
            "com-tam-long-xuyen       1.00      1.00      1.00        36\n",
            " bun-hai-san-be-be       0.97      0.93      0.95        30\n",
            " banh-hoi-heo-quay       0.94      1.00      0.97        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.97      0.95      0.96        37\n",
            "          mi-quang       0.89      0.94      0.91        33\n",
            "        bun-bo-hue       1.00      0.97      0.99        36\n",
            "        pho-ha-noi       0.97      1.00      0.99        36\n",
            "           bun-muc       1.00      0.85      0.92        13\n",
            "           bun-moc       0.95      1.00      0.97        19\n",
            "   bun-dau-mam-tom       1.00      0.92      0.96        25\n",
            "\n",
            "          accuracy                           0.95       380\n",
            "         macro avg       0.95      0.94      0.94       380\n",
            "      weighted avg       0.96      0.95      0.95       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.86\n",
            "bun-nuoc-leo: 0.73\n",
            "com-tam-long-xuyen: 1.00\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.95\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 0.97\n",
            "pho-ha-noi: 1.00\n",
            "bun-muc: 0.85\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating to Yolov8"
      ],
      "metadata": {
        "id": "O7yFdurW33n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO, RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/classes.txt'\n",
        "excel_file_path = '/content/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_yolov8 = YOLO('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/models/detector/yolov8_best_862.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "v8_2D2 = []\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global v8_2D2\n",
        "    img = cv2.imread(image_path)\n",
        "    yolov8_names, yolov8_detected_cls, yolov8_boxes = detect_ingredients(model_yolov8, img)\n",
        "    v8_rm_duplicates = set(yolov8_detected_cls)\n",
        "    v8_rm_duplicates = [int(number) for number in v8_rm_duplicates]\n",
        "    yolov8_detected_cls = list(v8_rm_duplicates)\n",
        "    v8_rm_duplicates = convertIdx2Class(v8_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v8_distances, v8_predicted_food = find_foodname_ver_narrow(v8_rm_duplicates)\n",
        "    print(v8_predicted_food)\n",
        "    v8_2D2.append(v8_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue', 'pho-ha-noi',\n",
        "               'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/'+category):\n",
        "            imagePaths.append(['/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)\n"
      ],
      "metadata": {
        "id": "Zh3AVvoG3MxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(v8_2D2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8TWAFEG3PWj",
        "outputId": "a19d68bf-bcd1-4a58-d1b4-10e3ece6c780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 8, 8, 11, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 8, 9, 9, 9, 10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, v8_2D2)\n",
        "precision = precision_score(labels, v8_2D2, average='weighted')\n",
        "recall = recall_score(labels, v8_2D2, average='weighted')\n",
        "f1 = f1_score(labels, v8_2D2, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, v8_2D2, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, v8_2D2)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD14Uuqx3Qk6",
        "outputId": "a4fee19d-5e3b-446e-8633-c46ceb1679a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.21052631578948\n",
            "Precision: 95.36190779367773\n",
            "Recall: 94.21052631578948\n",
            "F1 Score: 94.34501455419041\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.65      1.00      0.78        20\n",
            "    hu-tieu-my-tho       1.00      0.71      0.83        14\n",
            "      bun-nuoc-leo       1.00      0.77      0.87        26\n",
            "com-tam-long-xuyen       1.00      1.00      1.00        36\n",
            " bun-hai-san-be-be       1.00      0.93      0.97        30\n",
            " banh-hoi-heo-quay       0.94      1.00      0.97        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.97      0.92      0.94        37\n",
            "          mi-quang       0.86      0.94      0.90        33\n",
            "        bun-bo-hue       0.97      0.97      0.97        36\n",
            "        pho-ha-noi       0.97      0.97      0.97        36\n",
            "           bun-muc       0.92      0.92      0.92        13\n",
            "           bun-moc       0.95      1.00      0.97        19\n",
            "   bun-dau-mam-tom       1.00      0.92      0.96        25\n",
            "\n",
            "          accuracy                           0.94       380\n",
            "         macro avg       0.95      0.93      0.93       380\n",
            "      weighted avg       0.95      0.94      0.94       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.71\n",
            "bun-nuoc-leo: 0.77\n",
            "com-tam-long-xuyen: 1.00\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.92\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 0.97\n",
            "pho-ha-noi: 0.97\n",
            "bun-muc: 0.92\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating to Yolov9"
      ],
      "metadata": {
        "id": "qxcRt6lm20IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO, RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/classes.txt'\n",
        "excel_file_path = '/content/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_yolov9 = YOLO('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/models/detector/yolov9_best_85.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "v9_2D2 = []\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global v9_2D2\n",
        "    img = cv2.imread(image_path)\n",
        "    yolov9_names, yolov9_detected_cls, yolov9_boxes = detect_ingredients(model_yolov9, img)\n",
        "    v9_rm_duplicates = set(yolov9_detected_cls)\n",
        "    v9_rm_duplicates = [int(number) for number in v9_rm_duplicates]\n",
        "    yolov9_detected_cls = list(v9_rm_duplicates)\n",
        "    v9_rm_duplicates = convertIdx2Class(v9_rm_duplicates, classes_ingre)\n",
        "\n",
        "    v9_distances, v9_predicted_food = find_foodname_ver_narrow(v9_rm_duplicates)\n",
        "    print(v9_predicted_food)\n",
        "    v9_2D2.append(v9_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue', 'pho-ha-noi',\n",
        "               'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/'+category):\n",
        "            imagePaths.append(['/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)\n",
        "    print(v9_2D2)\n",
        "\n"
      ],
      "metadata": {
        "id": "1PQfezJYy5zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, v9_2D2)\n",
        "precision = precision_score(labels, v9_2D2, average='weighted')\n",
        "recall = recall_score(labels, v9_2D2, average='weighted')\n",
        "f1 = f1_score(labels, v9_2D2, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, v9_2D2, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, v9_2D2)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdYQkM-p0U0k",
        "outputId": "c6a77c1a-7aa0-4e6f-99b1-eb99ea1156a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.73684210526315\n",
            "Precision: 95.68080449535817\n",
            "Recall: 94.73684210526315\n",
            "F1 Score: 94.80000812820455\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.69      1.00      0.82        20\n",
            "    hu-tieu-my-tho       1.00      0.79      0.88        14\n",
            "      bun-nuoc-leo       1.00      0.73      0.84        26\n",
            "com-tam-long-xuyen       1.00      1.00      1.00        36\n",
            " bun-hai-san-be-be       1.00      0.93      0.97        30\n",
            " banh-hoi-heo-quay       0.94      1.00      0.97        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.97      0.92      0.94        37\n",
            "          mi-quang       0.86      0.94      0.90        33\n",
            "        bun-bo-hue       0.95      1.00      0.97        36\n",
            "        pho-ha-noi       1.00      0.94      0.97        36\n",
            "           bun-muc       0.87      1.00      0.93        13\n",
            "           bun-moc       1.00      1.00      1.00        19\n",
            "   bun-dau-mam-tom       1.00      0.96      0.98        25\n",
            "\n",
            "          accuracy                           0.95       380\n",
            "         macro avg       0.95      0.94      0.94       380\n",
            "      weighted avg       0.96      0.95      0.95       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.79\n",
            "bun-nuoc-leo: 0.73\n",
            "com-tam-long-xuyen: 1.00\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.92\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 1.00\n",
            "pho-ha-noi: 0.94\n",
            "bun-muc: 1.00\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating to RT-DETR"
      ],
      "metadata": {
        "id": "HQJ-V2B44M4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import RTDETR\n",
        "import cv2\n",
        "import math\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Đọc file cấu hình\n",
        "classes_path = '/content/classes.txt'\n",
        "excel_file_path = '/content/rules.xlsx'\n",
        "\n",
        "# Tải các mô hình\n",
        "model_rtdetr = RTDETR('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/models/detector/rt-detr_best_865.pt')\n",
        "\n",
        "df = pd.read_excel(excel_file_path)\n",
        "classes_ingre = []\n",
        "\n",
        "with open(classes_path, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "    classes_ingre = lines\n",
        "\n",
        "def convertIdx2Class(set_index, classes):\n",
        "    result_set = {classes[item] for item in set_index}\n",
        "    return result_set\n",
        "\n",
        "def preprocess_dataframe():\n",
        "    global df\n",
        "    normalized_df = df.drop(df.columns[[0, -1]], axis=1)\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        for col in normalized_df.columns:\n",
        "            value = row[col]\n",
        "            if pd.notna(value):\n",
        "                name_without_accents = unidecode(value)\n",
        "                values = name_without_accents.split(', ')\n",
        "                for i in range(len(values)):\n",
        "                    values[i] = values[i].replace(' ', '-')\n",
        "                row[col] = ', '.join(values)\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "normalized_df = preprocess_dataframe()\n",
        "\n",
        "def detect_ingredients(model, img):\n",
        "    results = model(img, conf=0.3, save=False)\n",
        "    names = results[0].names\n",
        "    detected_cls = results[0].boxes.cls.tolist()\n",
        "    boxes = results[0].boxes.xyxy.tolist()\n",
        "    return names, detected_cls, boxes\n",
        "\n",
        "rtdetr_2D2 = []\n",
        "\n",
        "def find_foodname_ver_narrow(set_detected_ingre):\n",
        "    global normalized_df\n",
        "    global df\n",
        "    distances = []\n",
        "\n",
        "    for index, row in normalized_df.iterrows():\n",
        "        values = {}\n",
        "        values[\"prior\"] = set(row[\"Priorities\"].split(', '))\n",
        "        values[\"main\"] = set(row[\"Ingredients\"].split(', '))\n",
        "        values[\"extra\"] = set(row[\"Secondary Ingredients\"].split(', ')) if pd.notna(row[\"Secondary Ingredients\"]) else set()\n",
        "\n",
        "        # Không khớp với ƯU TIÊN\n",
        "        no_match_prior = values[\"prior\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Không khớp với CHÍNH\n",
        "        no_match_main = values[\"main\"].difference(set_detected_ingre)\n",
        "\n",
        "        # Khớp với PHỤ\n",
        "        match_extra = values[\"extra\"].intersection(set_detected_ingre)\n",
        "\n",
        "        # Phần tử thuộc detect nhưng không thuộc bộ luật CHÍNH & PHỤ\n",
        "        combine = values[\"prior\"].union(values[\"main\"])\n",
        "        combine = combine.union(values[\"extra\"])\n",
        "        redundancy = set_detected_ingre.difference(combine)\n",
        "\n",
        "        shortage_score = (len(no_match_prior) * 10) + len(no_match_main) - (len(match_extra)*0.5)\n",
        "        redundancy_score = len(redundancy)\n",
        "        score = shortage_score + redundancy_score\n",
        "\n",
        "        distances.append(score)\n",
        "\n",
        "    min_value = min(distances)\n",
        "    min_indices = [i for i, value in enumerate(distances) if value == min_value]\n",
        "    return distances, min_indices[0]\n",
        "\n",
        "\n",
        "def recognize_food(image_path):\n",
        "    global rtdetr_2D2\n",
        "    img = cv2.imread(image_path)\n",
        "    rtdetr_names, rtdetr_detected_cls, rtdetr_boxes = detect_ingredients(model_rtdetr, img)\n",
        "    rtdetr_rm_duplicates = set(rtdetr_detected_cls)\n",
        "    rtdetr_rm_duplicates = [int(number) for number in rtdetr_rm_duplicates]\n",
        "    rtdetr_detected_cls = list(rtdetr_rm_duplicates)\n",
        "    rtdetr_rm_duplicates = convertIdx2Class(rtdetr_rm_duplicates, classes_ingre)\n",
        "\n",
        "    rtdetr_distances, rtdetr_predicted_food = find_foodname_ver_narrow(rtdetr_rm_duplicates)\n",
        "    print(rtdetr_predicted_food)\n",
        "    rtdetr_2D2.append(rtdetr_predicted_food)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = []# tấm hình\n",
        "    labels = []# nhãn\n",
        "    imagePaths = []\n",
        "    folders = ['bun-ca', 'hu-tieu-my-tho', 'bun-nuoc-leo', 'com-tam-long-xuyen', 'bun-hai-san-be-be',\n",
        "               'banh-hoi-heo-quay', 'com-ga', 'cao-lau', 'mi-quang', 'bun-bo-hue', 'pho-ha-noi',\n",
        "               'bun-muc', 'bun-moc', 'bun-dau-mam-tom']\n",
        "\n",
        "    for k, category in enumerate(folders):\n",
        "        for f in os.listdir('/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/'+category):\n",
        "            imagePaths.append(['/content/drive/MyDrive/My_Research/ViFLAVOR_Belief-merging/dataset_classification/data_test/' + category+'/'+f, k])\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "        data = imagePath[0]\n",
        "        label = imagePath[1]\n",
        "\n",
        "        images.append(data)\n",
        "        labels.append(label)\n",
        "\n",
        "    for img in images:\n",
        "        recognize_food(img)\n"
      ],
      "metadata": {
        "id": "Xo_GgHB24B57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rtdetr_2D2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQpYJTHt469Y",
        "outputId": "dc407919-d213-43dc-dba7-d34bbfc088a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 8, 1, 1, 8, 1, 1, 1, 1, 1, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 0, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 12, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính các chỉ số đánh giá\n",
        "accuracy = accuracy_score(labels, rtdetr_2D2)\n",
        "precision = precision_score(labels, rtdetr_2D2, average='weighted')\n",
        "recall = recall_score(labels, rtdetr_2D2, average='weighted')\n",
        "f1 = f1_score(labels, rtdetr_2D2, average='weighted')\n",
        "\n",
        "# In ra kết quả\n",
        "print('Accuracy:', accuracy * 100)\n",
        "print('Precision:', precision * 100)\n",
        "print('Recall:', recall * 100)\n",
        "print('F1 Score:', f1* 100)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(labels, rtdetr_2D2, target_names=folders))\n",
        "\n",
        "# Tính accuracy cho từng nhãn\n",
        "conf_matrix = confusion_matrix(labels, rtdetr_2D2)\n",
        "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for class_name, class_accuracy in zip(folders, class_accuracies):\n",
        "    print(f'{class_name}: {class_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na_0Ec3F5Ovr",
        "outputId": "98279132-df3d-4f38-d3a6-27eeae958cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.73684210526315\n",
            "Precision: 95.93324181566966\n",
            "Recall: 94.73684210526315\n",
            "F1 Score: 94.97862532756365\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "            bun-ca       0.65      1.00      0.78        20\n",
            "    hu-tieu-my-tho       1.00      0.86      0.92        14\n",
            "      bun-nuoc-leo       1.00      0.81      0.89        26\n",
            "com-tam-long-xuyen       1.00      0.97      0.99        36\n",
            " bun-hai-san-be-be       1.00      0.93      0.97        30\n",
            " banh-hoi-heo-quay       1.00      1.00      1.00        15\n",
            "            com-ga       1.00      1.00      1.00        40\n",
            "           cao-lau       0.94      0.92      0.93        37\n",
            "          mi-quang       0.84      0.94      0.89        33\n",
            "        bun-bo-hue       1.00      0.94      0.97        36\n",
            "        pho-ha-noi       1.00      1.00      1.00        36\n",
            "           bun-muc       1.00      0.92      0.96        13\n",
            "           bun-moc       0.95      1.00      0.97        19\n",
            "   bun-dau-mam-tom       1.00      0.92      0.96        25\n",
            "\n",
            "          accuracy                           0.95       380\n",
            "         macro avg       0.96      0.94      0.95       380\n",
            "      weighted avg       0.96      0.95      0.95       380\n",
            "\n",
            "\n",
            "Accuracy for each class:\n",
            "bun-ca: 1.00\n",
            "hu-tieu-my-tho: 0.86\n",
            "bun-nuoc-leo: 0.81\n",
            "com-tam-long-xuyen: 0.97\n",
            "bun-hai-san-be-be: 0.93\n",
            "banh-hoi-heo-quay: 1.00\n",
            "com-ga: 1.00\n",
            "cao-lau: 0.92\n",
            "mi-quang: 0.94\n",
            "bun-bo-hue: 0.94\n",
            "pho-ha-noi: 1.00\n",
            "bun-muc: 0.92\n",
            "bun-moc: 1.00\n",
            "bun-dau-mam-tom: 0.92\n"
          ]
        }
      ]
    }
  ]
}